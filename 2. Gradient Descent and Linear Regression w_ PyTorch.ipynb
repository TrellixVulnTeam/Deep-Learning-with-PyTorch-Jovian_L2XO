{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Gradient Descent and Linear Regression w/ PyTorch.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtz8IKwm0i/j8cONB7pyh5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bSAEWNhtlV6F"},"outputs":[],"source":["!pip install wandb\n","!wandb login"]},{"cell_type":"markdown","source":["## Gradient Descent From Scratch"],"metadata":{"id":"P5VETGsyyDeZ"}},{"cell_type":"code","source":["import numpy as np\n","import torch"],"metadata":{"id":"IQ-jsyaVlgdB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training Data"],"metadata":{"id":"-_lfH2GDyeON"}},{"cell_type":"code","source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43], \n","                   [91, 88, 64], \n","                   [87, 134, 58], \n","                   [102, 43, 37], \n","                   [69, 96, 70]], dtype='float32')\n","\n","# Targets (apples, oranges)\n","targets = np.array([[56, 70], \n","                    [81, 101], \n","                    [119, 133], \n","                    [22, 37], \n","                    [103, 119]], dtype='float32')"],"metadata":{"id":"znLgpjwCyXVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert inputs and targets to tensors\n","inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)\n","\n","print(inputs)\n","print(targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"es5c647ny4hy","executionInfo":{"status":"ok","timestamp":1648056309025,"user_tz":-60,"elapsed":29,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"396ba355-03cb-435c-a222-c2ccdb74e59f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]])\n","tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.]])\n"]}]},{"cell_type":"code","source":["# Create the weights and biases\n","w = torch.randn((2, 3), requires_grad=True)\n","b = torch.randn(2, requires_grad=True)\n","\n","print(w)\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lw6mN2i-zHL1","executionInfo":{"status":"ok","timestamp":1648056314966,"user_tz":-60,"elapsed":742,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"3a499fb9-a972-4622-d464-c9161988c8fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.5601, -0.3552, -0.5379],\n","        [-0.3850,  0.6177, -1.1442]], requires_grad=True)\n","tensor([-0.8196,  0.7227], requires_grad=True)\n"]}]},{"cell_type":"code","source":["def model(x):\n","  return x @ w.t() + b\n","\n","model(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dzxYoYazijP","executionInfo":{"status":"ok","timestamp":1648056319172,"user_tz":-60,"elapsed":677,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"1bfa5ee1-9e93-4471-ba47-6f24e7dc9fe9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ -6.8643, -35.1994],\n","        [-15.5391, -53.1865],\n","        [-30.8927, -16.3685],\n","        [ 21.1318, -54.3238],\n","        [-33.9307, -46.6400]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# MSE Loss - Mean Square Error\n","def mse(preds, targets):\n","  diff = preds - targets\n","  return torch.sum(diff*diff) / diff.numel()\n","\n","loss = mse(model(inputs), targets)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJBcc-w0zxVC","executionInfo":{"status":"ok","timestamp":1648056324142,"user_tz":-60,"elapsed":593,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"66c9a204-bc01-4d06-e6d7-1d81bade01f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14741.8281, grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["loss.backward()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":802},"id":"wtiPeiRy0Fur","executionInfo":{"status":"error","timestamp":1648056396544,"user_tz":-60,"elapsed":51,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"89db7c51-5ea7-4b46-fa8a-e97048a59a06"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}]},{"cell_type":"code","source":["loss.backward()\n","\n","with torch.no_grad():\n","  w -= w.grad * 1e-5\n","  b -= b.grad * 1e-5\n","  w.grad.zero_()\n","  b.grad.zero_()\n","\n","mse(model(inputs), targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSl5Gko_0ANy","executionInfo":{"status":"ok","timestamp":1647788118158,"user_tz":-60,"elapsed":2,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"df657a30-0fed-4dc2-856f-562206640bce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2710.2029, grad_fn=<DivBackward0>)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# Train for multiple epochs\n","for i in range(1000):\n","  preds = model(inputs)\n","  loss = mse(preds, targets)\n","  loss.backward()\n","\n","  with torch.no_grad():\n","    w -= w.grad * 1e-5\n","    b -= b.grad * 1e-5\n","    w.grad.zero_()\n","    b.grad.zero_()"],"metadata":{"id":"58sxn76l0leo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = model(inputs)\n","\n","print(preds)\n","print(targets)\n","print(mse(preds, targets))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SX4RDpE11QFq","executionInfo":{"status":"ok","timestamp":1647788619309,"user_tz":-60,"elapsed":4,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"3b3b6994-3be1-4c4c-b0b3-429c1df03f6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 57.0530,  70.3429],\n","        [ 82.2778, 100.6489],\n","        [118.7100, 132.9531],\n","        [ 21.0953,  37.0110],\n","        [101.9036, 119.1432]], grad_fn=<AddBackward0>)\n","tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.]])\n","tensor(0.5110, grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"7tGLjBGP14fp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#USING THE PYTORCH LIBRARY"],"metadata":{"id":"XzisIOLOLgtk"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"uoWE488VLlFA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Data"],"metadata":{"id":"CNT2o5qVU4Xh"}},{"cell_type":"code","source":["# Input (temp, rainfall, humidity)\n","inputs = np.array([[73, 67, 43], \n","                   [91, 88, 64], \n","                   [87, 134, 58], \n","                   [102, 43, 37], \n","                   [69, 96, 70], \n","                   [74, 66, 43], \n","                   [91, 87, 65], \n","                   [88, 134, 59], \n","                   [101, 44, 37], \n","                   [68, 96, 71], \n","                   [73, 66, 44], \n","                   [92, 87, 64], \n","                   [87, 135, 57], \n","                   [103, 43, 36], \n","                   [68, 97, 70]], \n","                  dtype='float32')\n","\n","# Targets (apples, oranges)\n","targets = np.array([[56, 70], \n","                    [81, 101], \n","                    [119, 133], \n","                    [22, 37], \n","                    [103, 119],\n","                    [57, 69], \n","                    [80, 102], \n","                    [118, 132], \n","                    [21, 38], \n","                    [104, 118], \n","                    [57, 69], \n","                    [82, 100], \n","                    [118, 134], \n","                    [20, 38], \n","                    [102, 120]], \n","                   dtype='float32')\n","\n","inputs = torch.from_numpy(inputs)\n","targets = torch.from_numpy(targets)\n","inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrdKnm-_L548","executionInfo":{"status":"ok","timestamp":1648047976845,"user_tz":-60,"elapsed":102,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"bcb14be2-dd4a-43f4-e46a-366b24371300"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.],\n","        [ 74.,  66.,  43.],\n","        [ 91.,  87.,  65.],\n","        [ 88., 134.,  59.],\n","        [101.,  44.,  37.],\n","        [ 68.,  96.,  71.],\n","        [ 73.,  66.,  44.],\n","        [ 92.,  87.,  64.],\n","        [ 87., 135.,  57.],\n","        [103.,  43.,  36.],\n","        [ 68.,  97.,  70.]])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[""],"metadata":{"id":"oWeeuv-rUztn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset and DataLoader"],"metadata":{"id":"VwPLAWH9U0ow"}},{"cell_type":"code","source":["train_ds = TensorDataset(inputs, targets)\n","train_ds[0:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTmrKfhrMJ5-","executionInfo":{"status":"ok","timestamp":1648047976848,"user_tz":-60,"elapsed":91,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"68003107-1d39-4892-f3aa-bdbd9602e375"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 73.,  67.,  43.],\n","         [ 91.,  88.,  64.],\n","         [ 87., 134.,  58.]]), tensor([[ 56.,  70.],\n","         [ 81., 101.],\n","         [119., 133.]]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["batch_size = 5\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True)"],"metadata":{"id":"mFhWjzIjMz7x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for xb, yb in train_dl:\n","  print(xb)\n","  print(yb)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMDTyugjNLik","executionInfo":{"status":"ok","timestamp":1648047976852,"user_tz":-60,"elapsed":89,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"90a51d9b-1749-40ae-ad53-26a84daaea1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[102.,  43.,  37.],\n","        [ 91.,  87.,  65.],\n","        [ 73.,  67.,  43.],\n","        [ 68.,  97.,  70.],\n","        [ 87., 134.,  58.]])\n","tensor([[ 22.,  37.],\n","        [ 80., 102.],\n","        [ 56.,  70.],\n","        [102., 120.],\n","        [119., 133.]])\n"]}]},{"cell_type":"markdown","source":["## nn.Linear"],"metadata":{"id":"e4Jl23CGVCXS"}},{"cell_type":"code","source":["# Define the model\n","model = nn.Linear(3,2)\n","\n","print(model.weight)\n","print(model.bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y_ztzu7KNYfd","executionInfo":{"status":"ok","timestamp":1648048063146,"user_tz":-60,"elapsed":459,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"f391ae4c-8dca-428c-dfa9-8b727701d598"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.1777,  0.0923, -0.4564],\n","        [-0.0064,  0.5677, -0.0191]], requires_grad=True)\n","Parameter containing:\n","tensor([-0.2730,  0.3262], requires_grad=True)\n"]}]},{"cell_type":"code","source":["print(list(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"641xt8igUEOy","executionInfo":{"status":"ok","timestamp":1648048067625,"user_tz":-60,"elapsed":513,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"1392bc0f-8009-4619-ec18-61081235023f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[ 0.1777,  0.0923, -0.4564],\n","        [-0.0064,  0.5677, -0.0191]], requires_grad=True), Parameter containing:\n","tensor([-0.2730,  0.3262], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["# Generate predictions\n","pred = model(inputs)\n","pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TWSn0-uTUc9o","executionInfo":{"status":"ok","timestamp":1648048138494,"user_tz":-60,"elapsed":433,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"f80f44d5-5b81-4bc2-81f5-6d78cabb3c2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ -0.7412,  37.0737],\n","        [ -5.1890,  48.4795],\n","        [  1.0841,  74.7327],\n","        [  4.9371,  23.3781],\n","        [-11.0997,  53.0474],\n","        [ -0.6557,  36.4996],\n","        [ -5.7377,  47.8928],\n","        [  0.8054,  74.7072],\n","        [  4.8516,  23.9522],\n","        [-11.7339,  53.0348],\n","        [ -1.2899,  36.4870],\n","        [ -5.1035,  47.9054],\n","        [  1.6328,  75.3194],\n","        [  5.5713,  23.3907],\n","        [-11.1852,  53.6215]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":[""],"metadata":{"id":"6XXC0B6XUuTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loss Function"],"metadata":{"id":"bshGJhm-VHne"}},{"cell_type":"code","source":["loss_fn = F.mse_loss\n","\n","loss = loss_fn(model(inputs), targets)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6Zn7tbFVJVb","executionInfo":{"status":"ok","timestamp":1648048313894,"user_tz":-60,"elapsed":523,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"2fb2614a-705a-453f-c3de-bd34659fa923"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4958.7705, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## Optimizer"],"metadata":{"id":"mWTTupCsVfBw"}},{"cell_type":"code","source":["opt = torch.optim.SGD(model.parameters(), lr=1e-5)"],"metadata":{"id":"S3u346nrVU-7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Loop"],"metadata":{"id":"dkMVvHHwVppO"}},{"cell_type":"code","source":["# Utility function to train the model\n","\n","def fit(num_epochs, model, loss_fn, opt, train_dl):\n","  for epoch in range(num_epochs):\n","    for xb, yb in train_dl:\n","      pred = model(xb)\n","      loss = loss_fn(pred, yb)\n","      loss.backward()\n","      opt.step()\n","      opt.zero_grad()\n","    if (epoch+1) % 10 == 0:\n","      # print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {loss.item()}\")\n","      print('Epoch [{}/{}] Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"],"metadata":{"id":"g4L0xi-PVowK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fit(100, model, loss_fn, opt, train_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVnsIhROXnsn","executionInfo":{"status":"ok","timestamp":1648049165988,"user_tz":-60,"elapsed":490,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"3b3633ba-71f1-43a1-db77-983b281be81e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/100] Loss: 29.6115\n","Epoch [20/100] Loss: 27.1717\n","Epoch [30/100] Loss: 13.9419\n","Epoch [40/100] Loss: 21.2669\n","Epoch [50/100] Loss: 28.7550\n","Epoch [60/100] Loss: 15.6598\n","Epoch [70/100] Loss: 23.6873\n","Epoch [80/100] Loss: 8.9403\n","Epoch [90/100] Loss: 11.1339\n","Epoch [100/100] Loss: 14.0685\n"]}]},{"cell_type":"code","source":["pred = model(inputs)\n","pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4o_h8NJXvF_","executionInfo":{"status":"ok","timestamp":1648049167815,"user_tz":-60,"elapsed":20,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"17f604c6-7137-4404-9351-3506bbc31215"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 57.1674,  70.7143],\n","        [ 79.5165,  98.2914],\n","        [123.3093, 137.4258],\n","        [ 22.2770,  39.0169],\n","        [ 96.5972, 113.9173],\n","        [ 55.8730,  69.5870],\n","        [ 78.9130,  97.9740],\n","        [123.3555, 137.7989],\n","        [ 23.5714,  40.1442],\n","        [ 97.2882, 114.7273],\n","        [ 56.5639,  70.3970],\n","        [ 78.2220,  97.1640],\n","        [123.9128, 137.7432],\n","        [ 21.5860,  38.2069],\n","        [ 97.8917, 115.0447]], grad_fn=<AddmmBackward0>)"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pokI6NGnYgsq","executionInfo":{"status":"ok","timestamp":1648049167816,"user_tz":-60,"elapsed":17,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"746da53b-9dd3-4b16-d022-925fd9d40210"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 56.,  70.],\n","        [ 81., 101.],\n","        [119., 133.],\n","        [ 22.,  37.],\n","        [103., 119.],\n","        [ 57.,  69.],\n","        [ 80., 102.],\n","        [118., 132.],\n","        [ 21.,  38.],\n","        [104., 118.],\n","        [ 57.,  69.],\n","        [ 82., 100.],\n","        [118., 134.],\n","        [ 20.,  38.],\n","        [102., 120.]])"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["loss_fn(pred, targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8q8uzSlKX2wR","executionInfo":{"status":"ok","timestamp":1648049167818,"user_tz":-60,"elapsed":13,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"b8376535-9f00-4a91-a4cb-c121a8fd3071"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(12.8749, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":[""],"metadata":{"id":"tYO2YgAqX-fS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Building a Feedforward Neural Network from our Linear Model"],"metadata":{"id":"P4AFs_mOgvS5"}},{"cell_type":"code","source":["model2 = nn.Sequential(\n","    nn.Linear(3, 4),\n","    nn.ReLU(),\n","    nn.Linear(4, 2)\n",")\n","\n","opt2 = torch.optim.SGD(model2.parameters(), lr=1e-7)"],"metadata":{"id":"y2SpxUK3g4OJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fit(10000, model2, loss_fn, opt2, train_dl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eHhIlS8IhWu2","executionInfo":{"status":"ok","timestamp":1648051874179,"user_tz":-60,"elapsed":14699,"user":{"displayName":"Adejumo Daniel","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02925977078148845759"}},"outputId":"fefb990c-cc16-4c45-ce68-71787f7e9a87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/10000] Loss: 9596.5059\n","Epoch [20/10000] Loss: 6596.1357\n","Epoch [30/10000] Loss: 7169.8384\n","Epoch [40/10000] Loss: 4291.6240\n","Epoch [50/10000] Loss: 6532.5894\n","Epoch [60/10000] Loss: 6468.0889\n","Epoch [70/10000] Loss: 5323.3232\n","Epoch [80/10000] Loss: 4118.2627\n","Epoch [90/10000] Loss: 5662.6284\n","Epoch [100/10000] Loss: 4849.6343\n","Epoch [110/10000] Loss: 3194.0813\n","Epoch [120/10000] Loss: 2234.1064\n","Epoch [130/10000] Loss: 5260.9492\n","Epoch [140/10000] Loss: 3953.0669\n","Epoch [150/10000] Loss: 2502.5886\n","Epoch [160/10000] Loss: 2969.1201\n","Epoch [170/10000] Loss: 2932.8374\n","Epoch [180/10000] Loss: 2575.2141\n","Epoch [190/10000] Loss: 2260.4727\n","Epoch [200/10000] Loss: 562.6266\n","Epoch [210/10000] Loss: 3372.8926\n","Epoch [220/10000] Loss: 1273.5802\n","Epoch [230/10000] Loss: 1575.4902\n","Epoch [240/10000] Loss: 968.1326\n","Epoch [250/10000] Loss: 2152.1296\n","Epoch [260/10000] Loss: 1593.4084\n","Epoch [270/10000] Loss: 783.8942\n","Epoch [280/10000] Loss: 473.3074\n","Epoch [290/10000] Loss: 1022.4686\n","Epoch [300/10000] Loss: 1073.4625\n","Epoch [310/10000] Loss: 740.5051\n","Epoch [320/10000] Loss: 1283.3040\n","Epoch [330/10000] Loss: 824.8509\n","Epoch [340/10000] Loss: 524.5746\n","Epoch [350/10000] Loss: 714.2513\n","Epoch [360/10000] Loss: 715.4772\n","Epoch [370/10000] Loss: 973.0573\n","Epoch [380/10000] Loss: 727.8989\n","Epoch [390/10000] Loss: 787.0787\n","Epoch [400/10000] Loss: 865.1757\n","Epoch [410/10000] Loss: 582.6427\n","Epoch [420/10000] Loss: 468.0592\n","Epoch [430/10000] Loss: 606.2725\n","Epoch [440/10000] Loss: 465.2650\n","Epoch [450/10000] Loss: 457.9395\n","Epoch [460/10000] Loss: 531.5111\n","Epoch [470/10000] Loss: 543.5192\n","Epoch [480/10000] Loss: 854.3817\n","Epoch [490/10000] Loss: 199.9225\n","Epoch [500/10000] Loss: 645.1852\n","Epoch [510/10000] Loss: 788.3237\n","Epoch [520/10000] Loss: 677.8007\n","Epoch [530/10000] Loss: 636.6503\n","Epoch [540/10000] Loss: 632.2629\n","Epoch [550/10000] Loss: 223.4330\n","Epoch [560/10000] Loss: 485.2603\n","Epoch [570/10000] Loss: 501.4259\n","Epoch [580/10000] Loss: 193.6419\n","Epoch [590/10000] Loss: 833.9118\n","Epoch [600/10000] Loss: 359.2584\n","Epoch [610/10000] Loss: 168.1624\n","Epoch [620/10000] Loss: 582.6735\n","Epoch [630/10000] Loss: 609.1522\n","Epoch [640/10000] Loss: 324.2179\n","Epoch [650/10000] Loss: 492.6213\n","Epoch [660/10000] Loss: 744.8794\n","Epoch [670/10000] Loss: 170.0919\n","Epoch [680/10000] Loss: 492.4927\n","Epoch [690/10000] Loss: 563.3768\n","Epoch [700/10000] Loss: 506.5856\n","Epoch [710/10000] Loss: 600.7629\n","Epoch [720/10000] Loss: 495.3411\n","Epoch [730/10000] Loss: 433.7482\n","Epoch [740/10000] Loss: 781.5511\n","Epoch [750/10000] Loss: 750.8477\n","Epoch [760/10000] Loss: 711.9302\n","Epoch [770/10000] Loss: 662.5065\n","Epoch [780/10000] Loss: 761.2562\n","Epoch [790/10000] Loss: 515.0315\n","Epoch [800/10000] Loss: 318.5470\n","Epoch [810/10000] Loss: 546.5845\n","Epoch [820/10000] Loss: 188.8483\n","Epoch [830/10000] Loss: 150.2621\n","Epoch [840/10000] Loss: 407.3261\n","Epoch [850/10000] Loss: 456.1821\n","Epoch [860/10000] Loss: 191.4241\n","Epoch [870/10000] Loss: 731.3408\n","Epoch [880/10000] Loss: 187.4965\n","Epoch [890/10000] Loss: 409.9961\n","Epoch [900/10000] Loss: 175.8901\n","Epoch [910/10000] Loss: 443.4696\n","Epoch [920/10000] Loss: 261.9890\n","Epoch [930/10000] Loss: 393.8061\n","Epoch [940/10000] Loss: 735.7405\n","Epoch [950/10000] Loss: 465.1190\n","Epoch [960/10000] Loss: 663.9308\n","Epoch [970/10000] Loss: 490.5302\n","Epoch [980/10000] Loss: 612.2261\n","Epoch [990/10000] Loss: 531.3028\n","Epoch [1000/10000] Loss: 211.5752\n","Epoch [1010/10000] Loss: 230.5269\n","Epoch [1020/10000] Loss: 710.1096\n","Epoch [1030/10000] Loss: 949.7861\n","Epoch [1040/10000] Loss: 346.0159\n","Epoch [1050/10000] Loss: 317.5527\n","Epoch [1060/10000] Loss: 517.1552\n","Epoch [1070/10000] Loss: 581.2821\n","Epoch [1080/10000] Loss: 338.7850\n","Epoch [1090/10000] Loss: 686.5721\n","Epoch [1100/10000] Loss: 175.6551\n","Epoch [1110/10000] Loss: 612.3890\n","Epoch [1120/10000] Loss: 341.4657\n","Epoch [1130/10000] Loss: 157.7756\n","Epoch [1140/10000] Loss: 375.9940\n","Epoch [1150/10000] Loss: 136.6525\n","Epoch [1160/10000] Loss: 189.1525\n","Epoch [1170/10000] Loss: 342.7504\n","Epoch [1180/10000] Loss: 424.5319\n","Epoch [1190/10000] Loss: 228.6271\n","Epoch [1200/10000] Loss: 348.4465\n","Epoch [1210/10000] Loss: 177.1024\n","Epoch [1220/10000] Loss: 320.2298\n","Epoch [1230/10000] Loss: 411.8022\n","Epoch [1240/10000] Loss: 442.3111\n","Epoch [1250/10000] Loss: 351.2666\n","Epoch [1260/10000] Loss: 188.1007\n","Epoch [1270/10000] Loss: 85.0235\n","Epoch [1280/10000] Loss: 411.8434\n","Epoch [1290/10000] Loss: 598.5093\n","Epoch [1300/10000] Loss: 466.2146\n","Epoch [1310/10000] Loss: 366.0793\n","Epoch [1320/10000] Loss: 153.8730\n","Epoch [1330/10000] Loss: 204.9800\n","Epoch [1340/10000] Loss: 532.5907\n","Epoch [1350/10000] Loss: 640.9662\n","Epoch [1360/10000] Loss: 576.8783\n","Epoch [1370/10000] Loss: 425.8565\n","Epoch [1380/10000] Loss: 387.4871\n","Epoch [1390/10000] Loss: 107.7669\n","Epoch [1400/10000] Loss: 392.9784\n","Epoch [1410/10000] Loss: 411.9993\n","Epoch [1420/10000] Loss: 529.0573\n","Epoch [1430/10000] Loss: 146.7861\n","Epoch [1440/10000] Loss: 110.3371\n","Epoch [1450/10000] Loss: 417.3914\n","Epoch [1460/10000] Loss: 73.3910\n","Epoch [1470/10000] Loss: 73.6690\n","Epoch [1480/10000] Loss: 129.3769\n","Epoch [1490/10000] Loss: 316.8849\n","Epoch [1500/10000] Loss: 397.0739\n","Epoch [1510/10000] Loss: 162.1373\n","Epoch [1520/10000] Loss: 576.2855\n","Epoch [1530/10000] Loss: 161.1313\n","Epoch [1540/10000] Loss: 302.0993\n","Epoch [1550/10000] Loss: 510.7862\n","Epoch [1560/10000] Loss: 300.4311\n","Epoch [1570/10000] Loss: 365.8159\n","Epoch [1580/10000] Loss: 335.3443\n","Epoch [1590/10000] Loss: 300.8476\n","Epoch [1600/10000] Loss: 691.9794\n","Epoch [1610/10000] Loss: 380.1419\n","Epoch [1620/10000] Loss: 405.2003\n","Epoch [1630/10000] Loss: 493.2279\n","Epoch [1640/10000] Loss: 369.3459\n","Epoch [1650/10000] Loss: 504.6214\n","Epoch [1660/10000] Loss: 448.8138\n","Epoch [1670/10000] Loss: 490.1034\n","Epoch [1680/10000] Loss: 359.9553\n","Epoch [1690/10000] Loss: 373.6940\n","Epoch [1700/10000] Loss: 69.2149\n","Epoch [1710/10000] Loss: 468.3675\n","Epoch [1720/10000] Loss: 459.8925\n","Epoch [1730/10000] Loss: 68.1705\n","Epoch [1740/10000] Loss: 491.7092\n","Epoch [1750/10000] Loss: 265.9505\n","Epoch [1760/10000] Loss: 158.0159\n","Epoch [1770/10000] Loss: 137.4078\n","Epoch [1780/10000] Loss: 89.3512\n","Epoch [1790/10000] Loss: 271.2047\n","Epoch [1800/10000] Loss: 329.6119\n","Epoch [1810/10000] Loss: 370.6740\n","Epoch [1820/10000] Loss: 332.0070\n","Epoch [1830/10000] Loss: 304.0782\n","Epoch [1840/10000] Loss: 621.2032\n","Epoch [1850/10000] Loss: 236.4213\n","Epoch [1860/10000] Loss: 332.9074\n","Epoch [1870/10000] Loss: 471.1669\n","Epoch [1880/10000] Loss: 236.9078\n","Epoch [1890/10000] Loss: 264.7248\n","Epoch [1900/10000] Loss: 310.6651\n","Epoch [1910/10000] Loss: 257.1572\n","Epoch [1920/10000] Loss: 650.1608\n","Epoch [1930/10000] Loss: 257.0113\n","Epoch [1940/10000] Loss: 275.8536\n","Epoch [1950/10000] Loss: 84.6980\n","Epoch [1960/10000] Loss: 417.0245\n","Epoch [1970/10000] Loss: 68.9509\n","Epoch [1980/10000] Loss: 221.2814\n","Epoch [1990/10000] Loss: 268.4236\n","Epoch [2000/10000] Loss: 301.5461\n","Epoch [2010/10000] Loss: 290.6232\n","Epoch [2020/10000] Loss: 449.0640\n","Epoch [2030/10000] Loss: 402.8192\n","Epoch [2040/10000] Loss: 427.3635\n","Epoch [2050/10000] Loss: 78.7976\n","Epoch [2060/10000] Loss: 112.0872\n","Epoch [2070/10000] Loss: 416.1032\n","Epoch [2080/10000] Loss: 89.5763\n","Epoch [2090/10000] Loss: 401.4007\n","Epoch [2100/10000] Loss: 242.9079\n","Epoch [2110/10000] Loss: 79.4937\n","Epoch [2120/10000] Loss: 423.3225\n","Epoch [2130/10000] Loss: 384.9044\n","Epoch [2140/10000] Loss: 115.7191\n","Epoch [2150/10000] Loss: 228.0848\n","Epoch [2160/10000] Loss: 147.0250\n","Epoch [2170/10000] Loss: 77.5577\n","Epoch [2180/10000] Loss: 63.0652\n","Epoch [2190/10000] Loss: 417.5819\n","Epoch [2200/10000] Loss: 259.2235\n","Epoch [2210/10000] Loss: 383.8191\n","Epoch [2220/10000] Loss: 412.7496\n","Epoch [2230/10000] Loss: 207.1912\n","Epoch [2240/10000] Loss: 225.2714\n","Epoch [2250/10000] Loss: 435.9281\n","Epoch [2260/10000] Loss: 251.0097\n","Epoch [2270/10000] Loss: 255.4152\n","Epoch [2280/10000] Loss: 384.8899\n","Epoch [2290/10000] Loss: 395.0008\n","Epoch [2300/10000] Loss: 394.7587\n","Epoch [2310/10000] Loss: 353.6537\n","Epoch [2320/10000] Loss: 402.3773\n","Epoch [2330/10000] Loss: 271.1888\n","Epoch [2340/10000] Loss: 247.7027\n","Epoch [2350/10000] Loss: 184.1662\n","Epoch [2360/10000] Loss: 188.7538\n","Epoch [2370/10000] Loss: 254.6448\n","Epoch [2380/10000] Loss: 127.1625\n","Epoch [2390/10000] Loss: 218.7410\n","Epoch [2400/10000] Loss: 234.5410\n","Epoch [2410/10000] Loss: 207.3217\n","Epoch [2420/10000] Loss: 207.1570\n","Epoch [2430/10000] Loss: 112.5046\n","Epoch [2440/10000] Loss: 112.5718\n","Epoch [2450/10000] Loss: 263.6697\n","Epoch [2460/10000] Loss: 184.2630\n","Epoch [2470/10000] Loss: 178.2036\n","Epoch [2480/10000] Loss: 198.4607\n","Epoch [2490/10000] Loss: 73.3051\n","Epoch [2500/10000] Loss: 176.0023\n","Epoch [2510/10000] Loss: 60.9115\n","Epoch [2520/10000] Loss: 342.6531\n","Epoch [2530/10000] Loss: 206.0883\n","Epoch [2540/10000] Loss: 213.9738\n","Epoch [2550/10000] Loss: 206.0623\n","Epoch [2560/10000] Loss: 89.7729\n","Epoch [2570/10000] Loss: 91.8771\n","Epoch [2580/10000] Loss: 105.4606\n","Epoch [2590/10000] Loss: 213.0671\n","Epoch [2600/10000] Loss: 201.6678\n","Epoch [2610/10000] Loss: 170.3954\n","Epoch [2620/10000] Loss: 215.2053\n","Epoch [2630/10000] Loss: 100.6150\n","Epoch [2640/10000] Loss: 199.8743\n","Epoch [2650/10000] Loss: 298.2006\n","Epoch [2660/10000] Loss: 76.0503\n","Epoch [2670/10000] Loss: 240.2220\n","Epoch [2680/10000] Loss: 233.1185\n","Epoch [2690/10000] Loss: 36.4560\n","Epoch [2700/10000] Loss: 224.9756\n","Epoch [2710/10000] Loss: 46.2881\n","Epoch [2720/10000] Loss: 207.6705\n","Epoch [2730/10000] Loss: 325.0794\n","Epoch [2740/10000] Loss: 56.0453\n","Epoch [2750/10000] Loss: 226.3036\n","Epoch [2760/10000] Loss: 161.9424\n","Epoch [2770/10000] Loss: 37.7117\n","Epoch [2780/10000] Loss: 25.7764\n","Epoch [2790/10000] Loss: 86.9347\n","Epoch [2800/10000] Loss: 151.7473\n","Epoch [2810/10000] Loss: 336.3602\n","Epoch [2820/10000] Loss: 31.7304\n","Epoch [2830/10000] Loss: 159.0500\n","Epoch [2840/10000] Loss: 55.9016\n","Epoch [2850/10000] Loss: 301.0058\n","Epoch [2860/10000] Loss: 142.7111\n","Epoch [2870/10000] Loss: 144.1744\n","Epoch [2880/10000] Loss: 85.5588\n","Epoch [2890/10000] Loss: 260.6914\n","Epoch [2900/10000] Loss: 55.0665\n","Epoch [2910/10000] Loss: 184.2615\n","Epoch [2920/10000] Loss: 86.7567\n","Epoch [2930/10000] Loss: 280.7635\n","Epoch [2940/10000] Loss: 170.8188\n","Epoch [2950/10000] Loss: 145.2341\n","Epoch [2960/10000] Loss: 58.1560\n","Epoch [2970/10000] Loss: 158.7635\n","Epoch [2980/10000] Loss: 280.2585\n","Epoch [2990/10000] Loss: 176.3339\n","Epoch [3000/10000] Loss: 160.9676\n","Epoch [3010/10000] Loss: 273.7965\n","Epoch [3020/10000] Loss: 142.7637\n","Epoch [3030/10000] Loss: 153.1844\n","Epoch [3040/10000] Loss: 119.0006\n","Epoch [3050/10000] Loss: 246.8736\n","Epoch [3060/10000] Loss: 128.6035\n","Epoch [3070/10000] Loss: 56.9280\n","Epoch [3080/10000] Loss: 161.9098\n","Epoch [3090/10000] Loss: 46.3906\n","Epoch [3100/10000] Loss: 347.8896\n","Epoch [3110/10000] Loss: 49.7659\n","Epoch [3120/10000] Loss: 257.1118\n","Epoch [3130/10000] Loss: 159.1183\n","Epoch [3140/10000] Loss: 289.8125\n","Epoch [3150/10000] Loss: 184.0773\n","Epoch [3160/10000] Loss: 147.6262\n","Epoch [3170/10000] Loss: 257.2956\n","Epoch [3180/10000] Loss: 143.7529\n","Epoch [3190/10000] Loss: 226.2441\n","Epoch [3200/10000] Loss: 125.9867\n","Epoch [3210/10000] Loss: 153.0341\n","Epoch [3220/10000] Loss: 57.8411\n","Epoch [3230/10000] Loss: 131.0255\n","Epoch [3240/10000] Loss: 141.1370\n","Epoch [3250/10000] Loss: 149.2102\n","Epoch [3260/10000] Loss: 172.6774\n","Epoch [3270/10000] Loss: 122.1050\n","Epoch [3280/10000] Loss: 176.3601\n","Epoch [3290/10000] Loss: 82.0689\n","Epoch [3300/10000] Loss: 140.4181\n","Epoch [3310/10000] Loss: 149.0461\n","Epoch [3320/10000] Loss: 151.8316\n","Epoch [3330/10000] Loss: 68.3823\n","Epoch [3340/10000] Loss: 231.5782\n","Epoch [3350/10000] Loss: 135.4679\n","Epoch [3360/10000] Loss: 236.0754\n","Epoch [3370/10000] Loss: 118.8020\n","Epoch [3380/10000] Loss: 135.4853\n","Epoch [3390/10000] Loss: 139.5047\n","Epoch [3400/10000] Loss: 17.9785\n","Epoch [3410/10000] Loss: 253.6709\n","Epoch [3420/10000] Loss: 104.4963\n","Epoch [3430/10000] Loss: 74.5482\n","Epoch [3440/10000] Loss: 161.1353\n","Epoch [3450/10000] Loss: 196.4211\n","Epoch [3460/10000] Loss: 109.5519\n","Epoch [3470/10000] Loss: 133.7502\n","Epoch [3480/10000] Loss: 129.6287\n","Epoch [3490/10000] Loss: 159.6877\n","Epoch [3500/10000] Loss: 129.5936\n","Epoch [3510/10000] Loss: 157.4124\n","Epoch [3520/10000] Loss: 106.8240\n","Epoch [3530/10000] Loss: 128.3534\n","Epoch [3540/10000] Loss: 221.5378\n","Epoch [3550/10000] Loss: 178.2453\n","Epoch [3560/10000] Loss: 121.8858\n","Epoch [3570/10000] Loss: 184.1805\n","Epoch [3580/10000] Loss: 233.0102\n","Epoch [3590/10000] Loss: 127.1189\n","Epoch [3600/10000] Loss: 104.6784\n","Epoch [3610/10000] Loss: 212.0040\n","Epoch [3620/10000] Loss: 130.6749\n","Epoch [3630/10000] Loss: 44.1235\n","Epoch [3640/10000] Loss: 143.1733\n","Epoch [3650/10000] Loss: 206.1460\n","Epoch [3660/10000] Loss: 140.4805\n","Epoch [3670/10000] Loss: 182.0497\n","Epoch [3680/10000] Loss: 198.6769\n","Epoch [3690/10000] Loss: 99.2282\n","Epoch [3700/10000] Loss: 190.7215\n","Epoch [3710/10000] Loss: 99.2512\n","Epoch [3720/10000] Loss: 199.6446\n","Epoch [3730/10000] Loss: 244.9046\n","Epoch [3740/10000] Loss: 89.4061\n","Epoch [3750/10000] Loss: 111.3829\n","Epoch [3760/10000] Loss: 189.7019\n","Epoch [3770/10000] Loss: 205.8312\n","Epoch [3780/10000] Loss: 167.1422\n","Epoch [3790/10000] Loss: 113.4594\n","Epoch [3800/10000] Loss: 35.5069\n","Epoch [3810/10000] Loss: 91.8721\n","Epoch [3820/10000] Loss: 185.0306\n","Epoch [3830/10000] Loss: 180.8752\n","Epoch [3840/10000] Loss: 111.3949\n","Epoch [3850/10000] Loss: 155.5946\n","Epoch [3860/10000] Loss: 161.7853\n","Epoch [3870/10000] Loss: 87.9111\n","Epoch [3880/10000] Loss: 109.8246\n","Epoch [3890/10000] Loss: 41.7478\n","Epoch [3900/10000] Loss: 58.1096\n","Epoch [3910/10000] Loss: 57.2655\n","Epoch [3920/10000] Loss: 106.8763\n","Epoch [3930/10000] Loss: 168.5216\n","Epoch [3940/10000] Loss: 159.1840\n","Epoch [3950/10000] Loss: 127.0874\n","Epoch [3960/10000] Loss: 127.4954\n","Epoch [3970/10000] Loss: 219.8778\n","Epoch [3980/10000] Loss: 30.7337\n","Epoch [3990/10000] Loss: 98.3850\n","Epoch [4000/10000] Loss: 116.2398\n","Epoch [4010/10000] Loss: 190.9380\n","Epoch [4020/10000] Loss: 212.7552\n","Epoch [4030/10000] Loss: 78.4383\n","Epoch [4040/10000] Loss: 122.2461\n","Epoch [4050/10000] Loss: 99.8325\n","Epoch [4060/10000] Loss: 140.9465\n","Epoch [4070/10000] Loss: 140.8132\n","Epoch [4080/10000] Loss: 33.7231\n","Epoch [4090/10000] Loss: 100.2770\n","Epoch [4100/10000] Loss: 114.4006\n","Epoch [4110/10000] Loss: 90.9603\n","Epoch [4120/10000] Loss: 95.7144\n","Epoch [4130/10000] Loss: 32.7504\n","Epoch [4140/10000] Loss: 94.8596\n","Epoch [4150/10000] Loss: 27.2492\n","Epoch [4160/10000] Loss: 90.3621\n","Epoch [4170/10000] Loss: 72.6492\n","Epoch [4180/10000] Loss: 95.0290\n","Epoch [4190/10000] Loss: 95.3212\n","Epoch [4200/10000] Loss: 89.4370\n","Epoch [4210/10000] Loss: 152.2676\n","Epoch [4220/10000] Loss: 71.6004\n","Epoch [4230/10000] Loss: 89.2536\n","Epoch [4240/10000] Loss: 166.9253\n","Epoch [4250/10000] Loss: 49.3096\n","Epoch [4260/10000] Loss: 32.1953\n","Epoch [4270/10000] Loss: 103.2096\n","Epoch [4280/10000] Loss: 35.8262\n","Epoch [4290/10000] Loss: 123.9151\n","Epoch [4300/10000] Loss: 141.6562\n","Epoch [4310/10000] Loss: 30.6371\n","Epoch [4320/10000] Loss: 12.3392\n","Epoch [4330/10000] Loss: 13.2202\n","Epoch [4340/10000] Loss: 121.4039\n","Epoch [4350/10000] Loss: 31.2057\n","Epoch [4360/10000] Loss: 125.6734\n","Epoch [4370/10000] Loss: 31.2753\n","Epoch [4380/10000] Loss: 142.1234\n","Epoch [4390/10000] Loss: 136.1275\n","Epoch [4400/10000] Loss: 31.8860\n","Epoch [4410/10000] Loss: 30.9908\n","Epoch [4420/10000] Loss: 31.9037\n","Epoch [4430/10000] Loss: 146.9169\n","Epoch [4440/10000] Loss: 78.0593\n","Epoch [4450/10000] Loss: 116.4015\n","Epoch [4460/10000] Loss: 63.2839\n","Epoch [4470/10000] Loss: 74.4333\n","Epoch [4480/10000] Loss: 75.2532\n","Epoch [4490/10000] Loss: 83.7830\n","Epoch [4500/10000] Loss: 75.3126\n","Epoch [4510/10000] Loss: 75.8029\n","Epoch [4520/10000] Loss: 85.2099\n","Epoch [4530/10000] Loss: 48.8578\n","Epoch [4540/10000] Loss: 26.1364\n","Epoch [4550/10000] Loss: 27.6910\n","Epoch [4560/10000] Loss: 127.8873\n","Epoch [4570/10000] Loss: 8.1563\n","Epoch [4580/10000] Loss: 72.8493\n","Epoch [4590/10000] Loss: 41.8964\n","Epoch [4600/10000] Loss: 105.0086\n","Epoch [4610/10000] Loss: 61.4342\n","Epoch [4620/10000] Loss: 75.4124\n","Epoch [4630/10000] Loss: 74.9585\n","Epoch [4640/10000] Loss: 102.2081\n","Epoch [4650/10000] Loss: 59.6209\n","Epoch [4660/10000] Loss: 51.9516\n","Epoch [4670/10000] Loss: 55.6813\n","Epoch [4680/10000] Loss: 86.9066\n","Epoch [4690/10000] Loss: 72.5750\n","Epoch [4700/10000] Loss: 66.6174\n","Epoch [4710/10000] Loss: 60.0047\n","Epoch [4720/10000] Loss: 56.3496\n","Epoch [4730/10000] Loss: 71.1112\n","Epoch [4740/10000] Loss: 56.7031\n","Epoch [4750/10000] Loss: 89.0521\n","Epoch [4760/10000] Loss: 68.9922\n","Epoch [4770/10000] Loss: 69.5331\n","Epoch [4780/10000] Loss: 99.6967\n","Epoch [4790/10000] Loss: 72.3580\n","Epoch [4800/10000] Loss: 57.0211\n","Epoch [4810/10000] Loss: 47.4407\n","Epoch [4820/10000] Loss: 37.7041\n","Epoch [4830/10000] Loss: 69.9054\n","Epoch [4840/10000] Loss: 40.6704\n","Epoch [4850/10000] Loss: 61.5354\n","Epoch [4860/10000] Loss: 37.0348\n","Epoch [4870/10000] Loss: 164.3575\n","Epoch [4880/10000] Loss: 125.9992\n","Epoch [4890/10000] Loss: 8.1276\n","Epoch [4900/10000] Loss: 22.3828\n","Epoch [4910/10000] Loss: 106.3567\n","Epoch [4920/10000] Loss: 70.8448\n","Epoch [4930/10000] Loss: 100.6013\n","Epoch [4940/10000] Loss: 103.3437\n","Epoch [4950/10000] Loss: 66.0165\n","Epoch [4960/10000] Loss: 50.9461\n","Epoch [4970/10000] Loss: 23.8296\n","Epoch [4980/10000] Loss: 24.1800\n","Epoch [4990/10000] Loss: 79.5180\n","Epoch [5000/10000] Loss: 119.3716\n","Epoch [5010/10000] Loss: 55.8897\n","Epoch [5020/10000] Loss: 66.4069\n","Epoch [5030/10000] Loss: 72.2051\n","Epoch [5040/10000] Loss: 37.5310\n","Epoch [5050/10000] Loss: 97.9555\n","Epoch [5060/10000] Loss: 5.5175\n","Epoch [5070/10000] Loss: 57.0856\n","Epoch [5080/10000] Loss: 51.3908\n","Epoch [5090/10000] Loss: 47.1483\n","Epoch [5100/10000] Loss: 22.9124\n","Epoch [5110/10000] Loss: 59.4453\n","Epoch [5120/10000] Loss: 75.9057\n","Epoch [5130/10000] Loss: 130.2063\n","Epoch [5140/10000] Loss: 97.2759\n","Epoch [5150/10000] Loss: 33.2357\n","Epoch [5160/10000] Loss: 19.7732\n","Epoch [5170/10000] Loss: 75.1929\n","Epoch [5180/10000] Loss: 94.4755\n","Epoch [5190/10000] Loss: 51.2761\n","Epoch [5200/10000] Loss: 35.7794\n","Epoch [5210/10000] Loss: 21.2226\n","Epoch [5220/10000] Loss: 66.8343\n","Epoch [5230/10000] Loss: 50.5588\n","Epoch [5240/10000] Loss: 66.8065\n","Epoch [5250/10000] Loss: 38.0546\n","Epoch [5260/10000] Loss: 52.3753\n","Epoch [5270/10000] Loss: 51.6374\n","Epoch [5280/10000] Loss: 22.0435\n","Epoch [5290/10000] Loss: 120.2054\n","Epoch [5300/10000] Loss: 54.4355\n","Epoch [5310/10000] Loss: 19.8289\n","Epoch [5320/10000] Loss: 86.4763\n","Epoch [5330/10000] Loss: 38.5374\n","Epoch [5340/10000] Loss: 55.1387\n","Epoch [5350/10000] Loss: 40.5039\n","Epoch [5360/10000] Loss: 20.6618\n","Epoch [5370/10000] Loss: 19.1451\n","Epoch [5380/10000] Loss: 36.8858\n","Epoch [5390/10000] Loss: 90.7097\n","Epoch [5400/10000] Loss: 81.8284\n","Epoch [5410/10000] Loss: 50.3939\n","Epoch [5420/10000] Loss: 89.5394\n","Epoch [5430/10000] Loss: 47.8175\n","Epoch [5440/10000] Loss: 34.9691\n","Epoch [5450/10000] Loss: 16.8455\n","Epoch [5460/10000] Loss: 33.6863\n","Epoch [5470/10000] Loss: 50.1769\n","Epoch [5480/10000] Loss: 19.2652\n","Epoch [5490/10000] Loss: 64.8789\n","Epoch [5500/10000] Loss: 31.6512\n","Epoch [5510/10000] Loss: 37.9122\n","Epoch [5520/10000] Loss: 60.2852\n","Epoch [5530/10000] Loss: 72.2623\n","Epoch [5540/10000] Loss: 46.5225\n","Epoch [5550/10000] Loss: 62.4926\n","Epoch [5560/10000] Loss: 79.6352\n","Epoch [5570/10000] Loss: 29.4389\n","Epoch [5580/10000] Loss: 90.6528\n","Epoch [5590/10000] Loss: 50.7072\n","Epoch [5600/10000] Loss: 76.4608\n","Epoch [5610/10000] Loss: 32.4680\n","Epoch [5620/10000] Loss: 49.1402\n","Epoch [5630/10000] Loss: 31.3152\n","Epoch [5640/10000] Loss: 82.1237\n","Epoch [5650/10000] Loss: 53.9951\n","Epoch [5660/10000] Loss: 6.6080\n","Epoch [5670/10000] Loss: 6.0997\n","Epoch [5680/10000] Loss: 7.0808\n","Epoch [5690/10000] Loss: 55.4329\n","Epoch [5700/10000] Loss: 55.8395\n","Epoch [5710/10000] Loss: 56.6734\n","Epoch [5720/10000] Loss: 47.0534\n","Epoch [5730/10000] Loss: 61.2764\n","Epoch [5740/10000] Loss: 54.4806\n","Epoch [5750/10000] Loss: 60.7388\n","Epoch [5760/10000] Loss: 42.2948\n","Epoch [5770/10000] Loss: 51.9036\n","Epoch [5780/10000] Loss: 40.6365\n","Epoch [5790/10000] Loss: 53.9066\n","Epoch [5800/10000] Loss: 54.0528\n","Epoch [5810/10000] Loss: 18.9329\n","Epoch [5820/10000] Loss: 69.3192\n","Epoch [5830/10000] Loss: 29.1494\n","Epoch [5840/10000] Loss: 31.8687\n","Epoch [5850/10000] Loss: 61.2417\n","Epoch [5860/10000] Loss: 41.5894\n","Epoch [5870/10000] Loss: 71.7943\n","Epoch [5880/10000] Loss: 53.0846\n","Epoch [5890/10000] Loss: 57.5469\n","Epoch [5900/10000] Loss: 27.3886\n","Epoch [5910/10000] Loss: 49.9891\n","Epoch [5920/10000] Loss: 55.1380\n","Epoch [5930/10000] Loss: 37.4693\n","Epoch [5940/10000] Loss: 59.4522\n","Epoch [5950/10000] Loss: 53.0349\n","Epoch [5960/10000] Loss: 26.8276\n","Epoch [5970/10000] Loss: 26.4159\n","Epoch [5980/10000] Loss: 14.9991\n","Epoch [5990/10000] Loss: 5.0535\n","Epoch [6000/10000] Loss: 7.8496\n","Epoch [6010/10000] Loss: 7.8736\n","Epoch [6020/10000] Loss: 30.4142\n","Epoch [6030/10000] Loss: 34.0652\n","Epoch [6040/10000] Loss: 60.2714\n","Epoch [6050/10000] Loss: 29.3666\n","Epoch [6060/10000] Loss: 58.1984\n","Epoch [6070/10000] Loss: 56.1089\n","Epoch [6080/10000] Loss: 27.1596\n","Epoch [6090/10000] Loss: 25.1474\n","Epoch [6100/10000] Loss: 5.8837\n","Epoch [6110/10000] Loss: 54.6209\n","Epoch [6120/10000] Loss: 50.3058\n","Epoch [6130/10000] Loss: 80.2264\n","Epoch [6140/10000] Loss: 17.0486\n","Epoch [6150/10000] Loss: 38.3671\n","Epoch [6160/10000] Loss: 34.3388\n","Epoch [6170/10000] Loss: 31.0443\n","Epoch [6180/10000] Loss: 35.4410\n","Epoch [6190/10000] Loss: 19.3150\n","Epoch [6200/10000] Loss: 32.8749\n","Epoch [6210/10000] Loss: 13.5305\n","Epoch [6220/10000] Loss: 44.4125\n","Epoch [6230/10000] Loss: 54.0677\n","Epoch [6240/10000] Loss: 7.1213\n","Epoch [6250/10000] Loss: 53.0140\n","Epoch [6260/10000] Loss: 48.7547\n","Epoch [6270/10000] Loss: 26.8557\n","Epoch [6280/10000] Loss: 14.5218\n","Epoch [6290/10000] Loss: 22.9320\n","Epoch [6300/10000] Loss: 22.4572\n","Epoch [6310/10000] Loss: 41.6097\n","Epoch [6320/10000] Loss: 56.8748\n","Epoch [6330/10000] Loss: 34.2241\n","Epoch [6340/10000] Loss: 37.1400\n","Epoch [6350/10000] Loss: 50.5471\n","Epoch [6360/10000] Loss: 34.2739\n","Epoch [6370/10000] Loss: 14.9065\n","Epoch [6380/10000] Loss: 15.3572\n","Epoch [6390/10000] Loss: 30.8885\n","Epoch [6400/10000] Loss: 30.5466\n","Epoch [6410/10000] Loss: 45.6468\n","Epoch [6420/10000] Loss: 40.4963\n","Epoch [6430/10000] Loss: 20.8907\n","Epoch [6440/10000] Loss: 42.5182\n","Epoch [6450/10000] Loss: 33.6606\n","Epoch [6460/10000] Loss: 16.5846\n","Epoch [6470/10000] Loss: 27.0067\n","Epoch [6480/10000] Loss: 48.0261\n","Epoch [6490/10000] Loss: 29.2811\n","Epoch [6500/10000] Loss: 20.6692\n","Epoch [6510/10000] Loss: 39.1060\n","Epoch [6520/10000] Loss: 49.3736\n","Epoch [6530/10000] Loss: 20.4464\n","Epoch [6540/10000] Loss: 40.0413\n","Epoch [6550/10000] Loss: 41.4632\n","Epoch [6560/10000] Loss: 41.7348\n","Epoch [6570/10000] Loss: 34.1408\n","Epoch [6580/10000] Loss: 29.8200\n","Epoch [6590/10000] Loss: 37.6460\n","Epoch [6600/10000] Loss: 22.3392\n","Epoch [6610/10000] Loss: 7.0786\n","Epoch [6620/10000] Loss: 25.9968\n","Epoch [6630/10000] Loss: 32.1766\n","Epoch [6640/10000] Loss: 36.0231\n","Epoch [6650/10000] Loss: 20.0939\n","Epoch [6660/10000] Loss: 39.7757\n","Epoch [6670/10000] Loss: 38.2165\n","Epoch [6680/10000] Loss: 38.1047\n","Epoch [6690/10000] Loss: 20.6056\n","Epoch [6700/10000] Loss: 33.3162\n","Epoch [6710/10000] Loss: 42.5950\n","Epoch [6720/10000] Loss: 16.0457\n","Epoch [6730/10000] Loss: 34.0540\n","Epoch [6740/10000] Loss: 33.9495\n","Epoch [6750/10000] Loss: 47.5104\n","Epoch [6760/10000] Loss: 15.9450\n","Epoch [6770/10000] Loss: 14.1475\n","Epoch [6780/10000] Loss: 28.4245\n","Epoch [6790/10000] Loss: 21.1943\n","Epoch [6800/10000] Loss: 31.7128\n","Epoch [6810/10000] Loss: 44.6928\n","Epoch [6820/10000] Loss: 22.2374\n","Epoch [6830/10000] Loss: 30.1236\n","Epoch [6840/10000] Loss: 25.8948\n","Epoch [6850/10000] Loss: 26.1106\n","Epoch [6860/10000] Loss: 41.7079\n","Epoch [6870/10000] Loss: 24.2369\n","Epoch [6880/10000] Loss: 5.9522\n","Epoch [6890/10000] Loss: 41.2571\n","Epoch [6900/10000] Loss: 39.7127\n","Epoch [6910/10000] Loss: 16.1239\n","Epoch [6920/10000] Loss: 24.0228\n","Epoch [6930/10000] Loss: 48.0892\n","Epoch [6940/10000] Loss: 36.5085\n","Epoch [6950/10000] Loss: 13.7337\n","Epoch [6960/10000] Loss: 37.7172\n","Epoch [6970/10000] Loss: 19.0536\n","Epoch [6980/10000] Loss: 36.1488\n","Epoch [6990/10000] Loss: 37.6812\n","Epoch [7000/10000] Loss: 24.7715\n","Epoch [7010/10000] Loss: 22.4975\n","Epoch [7020/10000] Loss: 27.4036\n","Epoch [7030/10000] Loss: 27.6148\n","Epoch [7040/10000] Loss: 24.3248\n","Epoch [7050/10000] Loss: 26.6353\n","Epoch [7060/10000] Loss: 19.6723\n","Epoch [7070/10000] Loss: 52.1936\n","Epoch [7080/10000] Loss: 25.5536\n","Epoch [7090/10000] Loss: 16.9288\n","Epoch [7100/10000] Loss: 34.1365\n","Epoch [7110/10000] Loss: 20.7098\n","Epoch [7120/10000] Loss: 20.8832\n","Epoch [7130/10000] Loss: 28.3080\n","Epoch [7140/10000] Loss: 15.7979\n","Epoch [7150/10000] Loss: 32.4968\n","Epoch [7160/10000] Loss: 27.1836\n","Epoch [7170/10000] Loss: 27.4452\n","Epoch [7180/10000] Loss: 39.8784\n","Epoch [7190/10000] Loss: 21.2836\n","Epoch [7200/10000] Loss: 32.1789\n","Epoch [7210/10000] Loss: 31.3380\n","Epoch [7220/10000] Loss: 40.0865\n","Epoch [7230/10000] Loss: 8.0526\n","Epoch [7240/10000] Loss: 19.6410\n","Epoch [7250/10000] Loss: 26.0322\n","Epoch [7260/10000] Loss: 38.6961\n","Epoch [7270/10000] Loss: 26.9928\n","Epoch [7280/10000] Loss: 28.3508\n","Epoch [7290/10000] Loss: 23.6774\n","Epoch [7300/10000] Loss: 21.1234\n","Epoch [7310/10000] Loss: 14.5812\n","Epoch [7320/10000] Loss: 17.5778\n","Epoch [7330/10000] Loss: 17.4648\n","Epoch [7340/10000] Loss: 27.1629\n","Epoch [7350/10000] Loss: 37.4951\n","Epoch [7360/10000] Loss: 19.5724\n","Epoch [7370/10000] Loss: 33.3060\n","Epoch [7380/10000] Loss: 13.2180\n","Epoch [7390/10000] Loss: 41.6520\n","Epoch [7400/10000] Loss: 27.2405\n","Epoch [7410/10000] Loss: 17.8900\n","Epoch [7420/10000] Loss: 35.1834\n","Epoch [7430/10000] Loss: 40.9505\n","Epoch [7440/10000] Loss: 19.3306\n","Epoch [7450/10000] Loss: 37.7367\n","Epoch [7460/10000] Loss: 17.3707\n","Epoch [7470/10000] Loss: 36.0764\n","Epoch [7480/10000] Loss: 20.4394\n","Epoch [7490/10000] Loss: 44.5809\n","Epoch [7500/10000] Loss: 6.5622\n","Epoch [7510/10000] Loss: 26.3175\n","Epoch [7520/10000] Loss: 17.7122\n","Epoch [7530/10000] Loss: 35.8414\n","Epoch [7540/10000] Loss: 30.3999\n","Epoch [7550/10000] Loss: 41.2357\n","Epoch [7560/10000] Loss: 22.3368\n","Epoch [7570/10000] Loss: 10.1360\n","Epoch [7580/10000] Loss: 12.0896\n","Epoch [7590/10000] Loss: 30.7079\n","Epoch [7600/10000] Loss: 31.9576\n","Epoch [7610/10000] Loss: 22.1399\n","Epoch [7620/10000] Loss: 25.3769\n","Epoch [7630/10000] Loss: 19.5059\n","Epoch [7640/10000] Loss: 25.8789\n","Epoch [7650/10000] Loss: 11.8595\n","Epoch [7660/10000] Loss: 25.6468\n","Epoch [7670/10000] Loss: 29.2699\n","Epoch [7680/10000] Loss: 14.2639\n","Epoch [7690/10000] Loss: 30.9681\n","Epoch [7700/10000] Loss: 22.7118\n","Epoch [7710/10000] Loss: 20.0786\n","Epoch [7720/10000] Loss: 8.7530\n","Epoch [7730/10000] Loss: 27.9485\n","Epoch [7740/10000] Loss: 13.6025\n","Epoch [7750/10000] Loss: 20.5505\n","Epoch [7760/10000] Loss: 23.3846\n","Epoch [7770/10000] Loss: 14.3760\n","Epoch [7780/10000] Loss: 31.2730\n","Epoch [7790/10000] Loss: 19.4409\n","Epoch [7800/10000] Loss: 26.0146\n","Epoch [7810/10000] Loss: 22.0670\n","Epoch [7820/10000] Loss: 37.2886\n","Epoch [7830/10000] Loss: 26.1636\n","Epoch [7840/10000] Loss: 25.3190\n","Epoch [7850/10000] Loss: 24.7512\n","Epoch [7860/10000] Loss: 22.1891\n","Epoch [7870/10000] Loss: 11.9535\n","Epoch [7880/10000] Loss: 24.6622\n","Epoch [7890/10000] Loss: 14.8830\n","Epoch [7900/10000] Loss: 38.8173\n","Epoch [7910/10000] Loss: 16.0347\n","Epoch [7920/10000] Loss: 17.9596\n","Epoch [7930/10000] Loss: 32.1477\n","Epoch [7940/10000] Loss: 22.4521\n","Epoch [7950/10000] Loss: 17.9906\n","Epoch [7960/10000] Loss: 28.1102\n","Epoch [7970/10000] Loss: 16.4650\n","Epoch [7980/10000] Loss: 12.8502\n","Epoch [7990/10000] Loss: 28.9348\n","Epoch [8000/10000] Loss: 21.7085\n","Epoch [8010/10000] Loss: 23.1193\n","Epoch [8020/10000] Loss: 15.1857\n","Epoch [8030/10000] Loss: 13.6218\n","Epoch [8040/10000] Loss: 22.3100\n","Epoch [8050/10000] Loss: 28.6543\n","Epoch [8060/10000] Loss: 31.7321\n","Epoch [8070/10000] Loss: 33.2090\n","Epoch [8080/10000] Loss: 12.6891\n","Epoch [8090/10000] Loss: 9.2731\n","Epoch [8100/10000] Loss: 25.7745\n","Epoch [8110/10000] Loss: 15.5948\n","Epoch [8120/10000] Loss: 23.6064\n","Epoch [8130/10000] Loss: 28.8672\n","Epoch [8140/10000] Loss: 22.9522\n","Epoch [8150/10000] Loss: 17.7512\n","Epoch [8160/10000] Loss: 12.5324\n","Epoch [8170/10000] Loss: 23.0496\n","Epoch [8180/10000] Loss: 32.5180\n","Epoch [8190/10000] Loss: 13.5029\n","Epoch [8200/10000] Loss: 16.3413\n","Epoch [8210/10000] Loss: 10.1950\n","Epoch [8220/10000] Loss: 21.5037\n","Epoch [8230/10000] Loss: 23.8310\n","Epoch [8240/10000] Loss: 14.4811\n","Epoch [8250/10000] Loss: 26.7541\n","Epoch [8260/10000] Loss: 18.8109\n","Epoch [8270/10000] Loss: 19.4354\n","Epoch [8280/10000] Loss: 20.3329\n","Epoch [8290/10000] Loss: 15.6729\n","Epoch [8300/10000] Loss: 24.9045\n","Epoch [8310/10000] Loss: 20.3473\n","Epoch [8320/10000] Loss: 21.6498\n","Epoch [8330/10000] Loss: 26.1944\n","Epoch [8340/10000] Loss: 16.2652\n","Epoch [8350/10000] Loss: 22.5718\n","Epoch [8360/10000] Loss: 24.4845\n","Epoch [8370/10000] Loss: 30.1253\n","Epoch [8380/10000] Loss: 16.8813\n","Epoch [8390/10000] Loss: 23.1454\n","Epoch [8400/10000] Loss: 26.8260\n","Epoch [8410/10000] Loss: 16.8284\n","Epoch [8420/10000] Loss: 16.7268\n","Epoch [8430/10000] Loss: 21.1346\n","Epoch [8440/10000] Loss: 17.2048\n","Epoch [8450/10000] Loss: 11.9426\n","Epoch [8460/10000] Loss: 23.8084\n","Epoch [8470/10000] Loss: 9.3828\n","Epoch [8480/10000] Loss: 14.9792\n","Epoch [8490/10000] Loss: 12.6320\n","Epoch [8500/10000] Loss: 14.7477\n","Epoch [8510/10000] Loss: 13.0573\n","Epoch [8520/10000] Loss: 17.7653\n","Epoch [8530/10000] Loss: 20.3316\n","Epoch [8540/10000] Loss: 18.9786\n","Epoch [8550/10000] Loss: 22.1935\n","Epoch [8560/10000] Loss: 22.7798\n","Epoch [8570/10000] Loss: 18.5270\n","Epoch [8580/10000] Loss: 19.8985\n","Epoch [8590/10000] Loss: 9.3724\n","Epoch [8600/10000] Loss: 12.9284\n","Epoch [8610/10000] Loss: 10.6148\n","Epoch [8620/10000] Loss: 24.9410\n","Epoch [8630/10000] Loss: 17.3019\n","Epoch [8640/10000] Loss: 12.5949\n","Epoch [8650/10000] Loss: 15.2025\n","Epoch [8660/10000] Loss: 13.2584\n","Epoch [8670/10000] Loss: 26.3985\n","Epoch [8680/10000] Loss: 19.0422\n","Epoch [8690/10000] Loss: 12.7790\n","Epoch [8700/10000] Loss: 13.4274\n","Epoch [8710/10000] Loss: 13.3760\n","Epoch [8720/10000] Loss: 14.1511\n","Epoch [8730/10000] Loss: 10.7770\n","Epoch [8740/10000] Loss: 12.2901\n","Epoch [8750/10000] Loss: 16.9888\n","Epoch [8760/10000] Loss: 18.0788\n","Epoch [8770/10000] Loss: 30.2874\n","Epoch [8780/10000] Loss: 16.1544\n","Epoch [8790/10000] Loss: 25.1591\n","Epoch [8800/10000] Loss: 19.0121\n","Epoch [8810/10000] Loss: 19.8527\n","Epoch [8820/10000] Loss: 17.2778\n","Epoch [8830/10000] Loss: 19.3874\n","Epoch [8840/10000] Loss: 12.0662\n","Epoch [8850/10000] Loss: 20.7465\n","Epoch [8860/10000] Loss: 5.6305\n","Epoch [8870/10000] Loss: 13.2866\n","Epoch [8880/10000] Loss: 13.5522\n","Epoch [8890/10000] Loss: 18.2592\n","Epoch [8900/10000] Loss: 26.2790\n","Epoch [8910/10000] Loss: 21.1580\n","Epoch [8920/10000] Loss: 13.5028\n","Epoch [8930/10000] Loss: 15.8694\n","Epoch [8940/10000] Loss: 12.1387\n","Epoch [8950/10000] Loss: 18.4751\n","Epoch [8960/10000] Loss: 5.3092\n","Epoch [8970/10000] Loss: 16.3981\n","Epoch [8980/10000] Loss: 14.5114\n","Epoch [8990/10000] Loss: 21.0345\n","Epoch [9000/10000] Loss: 17.8945\n","Epoch [9010/10000] Loss: 16.2859\n","Epoch [9020/10000] Loss: 26.2745\n","Epoch [9030/10000] Loss: 27.8277\n","Epoch [9040/10000] Loss: 19.5900\n","Epoch [9050/10000] Loss: 11.4732\n","Epoch [9060/10000] Loss: 18.5161\n","Epoch [9070/10000] Loss: 14.5431\n","Epoch [9080/10000] Loss: 12.0811\n","Epoch [9090/10000] Loss: 19.2055\n","Epoch [9100/10000] Loss: 15.0156\n","Epoch [9110/10000] Loss: 20.0576\n","Epoch [9120/10000] Loss: 16.0449\n","Epoch [9130/10000] Loss: 16.4770\n","Epoch [9140/10000] Loss: 18.5831\n","Epoch [9150/10000] Loss: 21.3598\n","Epoch [9160/10000] Loss: 13.7806\n","Epoch [9170/10000] Loss: 19.7990\n","Epoch [9180/10000] Loss: 16.6832\n","Epoch [9190/10000] Loss: 19.8529\n","Epoch [9200/10000] Loss: 21.2388\n","Epoch [9210/10000] Loss: 13.7928\n","Epoch [9220/10000] Loss: 16.1978\n","Epoch [9230/10000] Loss: 15.1005\n","Epoch [9240/10000] Loss: 24.6065\n","Epoch [9250/10000] Loss: 21.6412\n","Epoch [9260/10000] Loss: 21.8604\n","Epoch [9270/10000] Loss: 9.5780\n","Epoch [9280/10000] Loss: 9.4756\n","Epoch [9290/10000] Loss: 15.7934\n","Epoch [9300/10000] Loss: 17.1988\n","Epoch [9310/10000] Loss: 12.4594\n","Epoch [9320/10000] Loss: 16.8860\n","Epoch [9330/10000] Loss: 19.7971\n","Epoch [9340/10000] Loss: 17.4363\n","Epoch [9350/10000] Loss: 16.8066\n","Epoch [9360/10000] Loss: 13.3459\n","Epoch [9370/10000] Loss: 15.3183\n","Epoch [9380/10000] Loss: 13.6845\n","Epoch [9390/10000] Loss: 23.3701\n","Epoch [9400/10000] Loss: 14.7456\n","Epoch [9410/10000] Loss: 13.5371\n","Epoch [9420/10000] Loss: 11.3701\n","Epoch [9430/10000] Loss: 11.6260\n","Epoch [9440/10000] Loss: 21.4768\n","Epoch [9450/10000] Loss: 12.5389\n","Epoch [9460/10000] Loss: 18.7707\n","Epoch [9470/10000] Loss: 24.1762\n","Epoch [9480/10000] Loss: 10.4860\n","Epoch [9490/10000] Loss: 17.6310\n","Epoch [9500/10000] Loss: 15.9202\n","Epoch [9510/10000] Loss: 14.9076\n","Epoch [9520/10000] Loss: 23.4738\n","Epoch [9530/10000] Loss: 12.6630\n","Epoch [9540/10000] Loss: 24.0436\n","Epoch [9550/10000] Loss: 12.4579\n","Epoch [9560/10000] Loss: 13.6182\n","Epoch [9570/10000] Loss: 10.8447\n","Epoch [9580/10000] Loss: 14.8566\n","Epoch [9590/10000] Loss: 19.8367\n","Epoch [9600/10000] Loss: 23.6596\n","Epoch [9610/10000] Loss: 16.8473\n","Epoch [9620/10000] Loss: 10.6062\n","Epoch [9630/10000] Loss: 13.3849\n","Epoch [9640/10000] Loss: 15.8124\n","Epoch [9650/10000] Loss: 16.2843\n","Epoch [9660/10000] Loss: 12.6250\n","Epoch [9670/10000] Loss: 13.6283\n","Epoch [9680/10000] Loss: 14.1153\n","Epoch [9690/10000] Loss: 12.8929\n","Epoch [9700/10000] Loss: 8.2055\n","Epoch [9710/10000] Loss: 18.8410\n","Epoch [9720/10000] Loss: 18.1171\n","Epoch [9730/10000] Loss: 17.1164\n","Epoch [9740/10000] Loss: 12.1157\n","Epoch [9750/10000] Loss: 12.6086\n","Epoch [9760/10000] Loss: 17.5934\n","Epoch [9770/10000] Loss: 12.4874\n","Epoch [9780/10000] Loss: 20.7978\n","Epoch [9790/10000] Loss: 21.1466\n","Epoch [9800/10000] Loss: 23.7913\n","Epoch [9810/10000] Loss: 11.9604\n","Epoch [9820/10000] Loss: 10.0019\n","Epoch [9830/10000] Loss: 20.9982\n","Epoch [9840/10000] Loss: 18.4196\n","Epoch [9850/10000] Loss: 12.8299\n","Epoch [9860/10000] Loss: 15.1444\n","Epoch [9870/10000] Loss: 11.2442\n","Epoch [9880/10000] Loss: 6.0574\n","Epoch [9890/10000] Loss: 13.7749\n","Epoch [9900/10000] Loss: 19.2693\n","Epoch [9910/10000] Loss: 15.9940\n","Epoch [9920/10000] Loss: 18.4182\n","Epoch [9930/10000] Loss: 11.0536\n","Epoch [9940/10000] Loss: 16.9272\n","Epoch [9950/10000] Loss: 11.2452\n","Epoch [9960/10000] Loss: 17.5751\n","Epoch [9970/10000] Loss: 24.4076\n","Epoch [9980/10000] Loss: 17.4014\n","Epoch [9990/10000] Loss: 16.5618\n","Epoch [10000/10000] Loss: 8.8894\n"]}]},{"cell_type":"markdown","source":["## Blog Notes - Robotics Ignite\n","#### Series - Machine Learning to Reinforment Learning\n","#### Series - Matering ROS and Gazebo!\n","#### NLP, NLU, Everything Communication for AI!"],"metadata":{"id":"J7qSRA4Vjiqd"}},{"cell_type":"markdown","source":["\n","\n","1.   Getting staeted with Jupyter Notebooks! Your companion tool on the ML Journey\n","\n","    // getting free jupyter notes\n","    \n","    // setting up jupyter notes on your machine - Linux and Windows\n","\n","    // Mastering Markdown\n","    \n","    ---\n","\n","\n","\n","2.   PyTorch Basics! Your companion library on the ML Journey\n","\n","    // setting up pytorch - colab, kaggle, local\n","\n","    // PyTorch Basics. Autograd\n","\n","    ---\n","\n","3.  Linear Regression and Gradient Descent from Scratch! Your first ML Model\n","\n","\n"],"metadata":{"id":"hvZsFaBoj2D3"}},{"cell_type":"code","source":["## Include in your blog demonstrations of fitting the hyperparamters\n","## I had issues with the hyperparamters\n","## I had issues with the non-Linear activation function, and then variations \n","## in the learning rate towards convergence. Reducing the learning rate resulted\n","## in better convergeance"],"metadata":{"id":"1ldY9udahe9h"},"execution_count":null,"outputs":[]}]}